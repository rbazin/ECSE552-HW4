{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 : transformers applied to time series forecasting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_date_parser(date_string):\n",
    "    return pd.datetime.strptime(date_string, '%d.%m.%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/rbazin/ECSE552/ECSE552-HW4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_824786/649081062.py:2: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  return pd.datetime.strptime(date_string, '%d.%m.%Y %H:%M:%S')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 01:00:00</td>\n",
       "      <td>996.50</td>\n",
       "      <td>-8.05</td>\n",
       "      <td>265.38</td>\n",
       "      <td>-8.78</td>\n",
       "      <td>94.4</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.96</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1307.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.63</td>\n",
       "      <td>192.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 02:00:00</td>\n",
       "      <td>996.62</td>\n",
       "      <td>-8.88</td>\n",
       "      <td>264.54</td>\n",
       "      <td>-9.77</td>\n",
       "      <td>93.2</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1312.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>190.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 03:00:00</td>\n",
       "      <td>996.84</td>\n",
       "      <td>-8.81</td>\n",
       "      <td>264.59</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>93.5</td>\n",
       "      <td>3.13</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1312.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.63</td>\n",
       "      <td>167.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 04:00:00</td>\n",
       "      <td>996.99</td>\n",
       "      <td>-9.05</td>\n",
       "      <td>264.34</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>92.6</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1313.61</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.38</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 05:00:00</td>\n",
       "      <td>997.46</td>\n",
       "      <td>-9.63</td>\n",
       "      <td>263.72</td>\n",
       "      <td>-10.65</td>\n",
       "      <td>92.2</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1317.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.88</td>\n",
       "      <td>157.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
       "0 2009-01-01 01:00:00    996.50     -8.05    265.38        -8.78    94.4   \n",
       "1 2009-01-01 02:00:00    996.62     -8.88    264.54        -9.77    93.2   \n",
       "2 2009-01-01 03:00:00    996.84     -8.81    264.59        -9.66    93.5   \n",
       "3 2009-01-01 04:00:00    996.99     -9.05    264.34       -10.02    92.6   \n",
       "4 2009-01-01 05:00:00    997.46     -9.63    263.72       -10.65    92.2   \n",
       "\n",
       "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
       "0          3.33          3.14          0.19       1.96             3.15   \n",
       "1          3.12          2.90          0.21       1.81             2.91   \n",
       "2          3.13          2.93          0.20       1.83             2.94   \n",
       "3          3.07          2.85          0.23       1.78             2.85   \n",
       "4          2.94          2.71          0.23       1.69             2.71   \n",
       "\n",
       "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
       "0       1307.86      0.21           0.63     192.7  \n",
       "1       1312.25      0.25           0.63     190.3  \n",
       "2       1312.18      0.18           0.63     167.2  \n",
       "3       1313.61      0.10           0.38     240.0  \n",
       "4       1317.19      0.40           0.88     157.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(path, \"weather_train.csv\"), parse_dates = ['Date Time'], date_parser=custom_date_parser)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='Date Time', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'p (mbar)': 'p', 'T (degC)': 'T', 'rh (%)': 'rh', 'wv (m/s)': 'wv', 'Date Time': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['date'].apply(lambda x: x.replace(minute=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 56072 entries, 0 to 56071\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   date             56072 non-null  datetime64[ns]\n",
      " 1   p                56072 non-null  float64       \n",
      " 2   T                56072 non-null  float64       \n",
      " 3   Tpot (K)         56072 non-null  float64       \n",
      " 4   Tdew (degC)      56072 non-null  float64       \n",
      " 5   rh               56072 non-null  float64       \n",
      " 6   VPmax (mbar)     56072 non-null  float64       \n",
      " 7   VPact (mbar)     56072 non-null  float64       \n",
      " 8   VPdef (mbar)     56072 non-null  float64       \n",
      " 9   sh (g/kg)        56072 non-null  float64       \n",
      " 10  H2OC (mmol/mol)  56072 non-null  float64       \n",
      " 11  rho (g/m**3)     56072 non-null  float64       \n",
      " 12  wv               56072 non-null  float64       \n",
      " 13  max. wv (m/s)    56072 non-null  float64       \n",
      " 14  wd (deg)         56072 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(14)\n",
      "memory usage: 6.8 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build the architecture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # div_term = torch.exp(\n",
    "        #     torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        # )\n",
    "        div_term = 1 / (10000 ** ((2 * np.arange(d_model)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[0::2])\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[1::2])\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # [5000, 1, d_model],so need seq-len <= 5000\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.pe[:x.size(0), :].repeat(1,x.shape[1],1).shape ,'---',x.shape)\n",
    "        # dimension 1 maybe inequal batchsize\n",
    "        return x + self.pe[:x.size(0), :].repeat(1,x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFormer(nn.Module):\n",
    "    def __init__(self,feature_size=250,num_encoder_layers=1,dropout=0.1, nbr_labels=4):\n",
    "        super(TimeFormer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.input_embedding  = nn.Linear(nbr_labels, feature_size)\n",
    "        self.src_mask = None\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers, norm=None)\n",
    "        self.decoder = nn.Linear(feature_size * nbr_labels, nbr_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        # src with shape (input_window, batch_len, 1)\n",
    "        if self.src_mask is None or self.src_mask.size(0) != src.shape[1]:\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.input_embedding(src) # linear transformation before positional embedding\n",
    "        src = self.pos_encoder(src) # positional embedding\n",
    "        output = self.transformer_encoder(src,self.src_mask) # transformer encoder\n",
    "        output = output.reshape(output.shape[0], -1) # flatten the output\n",
    "        output = self.decoder(output)  # linear transformation decoder\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the a dataset class to encapsulate and format our data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFormerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for TimeFormer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_df, enc_seq_len=4, window_step=1, labels_as_features=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_seq_len = enc_seq_len\n",
    "        self.window_step = window_step\n",
    "\n",
    "        self.labels = torch.from_numpy(data_df.loc[:, [\"p\", \"T\", \"rh\", \"wv\"]].to_numpy()).type(torch.float32)\n",
    "        self.features = None\n",
    "        if not labels_as_features:\n",
    "            self.features = torch.from_numpy(data_df.loc[:, [k for k in data_df.columns if k not in [\"p\", \"T\", \"rh\", \"wv\", \"date\"]]].to_numpy()).type(torch.float32)\n",
    "\n",
    "        # list of tuple (start, end) of the sequences according to window_step and enc_seq_len\n",
    "        self.seq_list = [(i, i + enc_seq_len) for i in range(0, len(data_df) - enc_seq_len, window_step)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" \n",
    "        Returns the input and target sequences for a given index.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the start and end of the sequence\n",
    "        start, end = self.seq_list[idx]\n",
    "\n",
    "        # get the input sequence\n",
    "        src_seq = self.features[start:end] if self.features is not None else self.labels[start:end]\n",
    "        trgt_seq = self.labels[end]\n",
    "\n",
    "        return src_seq, trgt_seq.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(model, criterion, optimizer, epochs, train_loader, val_loader, device, save_path):\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    for i, epoch in enumerate(tqdm(range(epochs), desc=\"Epochs\")):\n",
    "\n",
    "        total_loss = 0.\n",
    "        model.train()\n",
    "        for batch in train_loader:  \n",
    "            \n",
    "            srcs = batch[0].to(device)\n",
    "            trgts = batch[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(srcs)\n",
    "            loss = criterion(output, trgts)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        # validation\n",
    "        total_loss = 0.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                srcs = batch[0].to(device)\n",
    "                trgts = batch[1].to(device)\n",
    "\n",
    "                output = model(srcs)\n",
    "                loss = criterion(output, trgts)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            val_losses.append(total_loss / len(val_loader))\n",
    "\n",
    "        tqdm.write(f\"Epoch {epoch+1} | Train loss: {train_losses[-1]:.4f} | Val loss: {val_losses[-1]:.4f}\")\n",
    "        \n",
    "        # checkpoint\n",
    "        if i % 10 == 0:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Training time: {end - start:.2f} seconds\")\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def evaluate(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        for batch in test_loader:\n",
    "            srcs = batch[0].to(device)\n",
    "            trgts = batch[1].to(device)\n",
    "\n",
    "            output = model(srcs)\n",
    "            loss = criterion(output, trgts)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "enc_seq_len = 4 # number of past observations to use\n",
    "window_step = 1 # step between two consecutive windows\n",
    "nbr_labels = 4 # number of labels to predict\n",
    "batch_size = 32\n",
    "dropout = 0.1\n",
    "num_encoder_layers = 1 # number of transformer encoder layers\n",
    "feature_size = 250 # size of the feature vector in the input embedding layer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets and dataloaders\n",
    "time_ds = TimeFormerDataset(data_df, enc_seq_len=enc_seq_len, window_step=window_step, labels_as_features=True)\n",
    "\n",
    "train_size = int(0.8 * len(time_ds)) # 80% for training\n",
    "val_size = int(0.1 * len(time_ds)) # 10% for validation\n",
    "test_size = len(time_ds) - train_size - val_size # remaining for test\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(time_ds, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "\n",
    "model = TimeFormer(nbr_labels=nbr_labels, feature_size=feature_size, num_encoder_layers=num_encoder_layers, dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model, train_losses, val_losses = train(model, criterion, optimizer, epochs, train_loader, val_loader, device, \"timeformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final losses | Train loss: {train_losses[-1]:.4f} | Val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(val_losses, label=\"Val loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, criterion, test_loader, device)\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
